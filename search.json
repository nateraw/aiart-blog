[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "aiart.dev",
    "section": "",
    "text": "Creating AI Music Videos with Stable Diffusion\n\n\n\n\n\nA tutorial on how to create AI music videos using the text-to-image model Stable Diffusion.\n\n\n\n\n\n\nOct 11, 2022\n\n\nNate Raw\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/sd-music-videos/sd_music_videos.html",
    "href": "posts/sd-music-videos/sd_music_videos.html",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "",
    "text": "%%capture\n! pip install diffusers==0.4.0 transformers"
  },
  {
    "objectID": "posts/sd-music-videos/sd_music_videos.html#interpolating-between-noise-vectors",
    "href": "posts/sd-music-videos/sd_music_videos.html#interpolating-between-noise-vectors",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "Interpolating Between Noise Vectors",
    "text": "Interpolating Between Noise Vectors\nNow let’s interpolate between images A and B as we described earlier…\n\nheight = 512\nwidth = 512\nnoise_shape = (1, 4, height // 8, width // 8)\n\nseed_a = 42\nseed_b = 5432\n\nnoise_a = torch.randn(\n    noise_shape,\n    generator=torch.Generator(\n        device=pipeline.device\n    ).manual_seed(seed_a),\n    device=pipeline.device,\n)\nnoise_b = torch.randn(\n    noise_shape,\n    generator=torch.Generator(\n        device=pipeline.device\n    ).manual_seed(seed_b),\n    device=pipeline.device\n)\n\n\n# Using same prompt for each image\nprompt = 'blueberry spaghetti'\n\n# Steps to interpolate (i.e. number of images to generate)\n# If you change the number here, the visualization below will break\nnum_interpolation_steps = 6\n\nT = torch.linspace(0.0, 1.0, num_interpolation_steps)\n\nimages = []\nfor i, t in enumerate(T):\n    print(f\"Weight at step {i} - {t:2.2f}\")\n    noise_t = slerp(float(t), noise_a, noise_b)\n    im = pipeline(prompt, latents=noise_t, height=height, width=width)['sample'][0]\n    images.append(im)\n\nLet’s visualize the frames we just created…\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nrows, cols = 2, 3\nfig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(15,15))\n\nfor r in range(rows):\n    for c in range(cols):\n        i = (r * cols) + c\n        axes[r, c].axis(\"off\")\n        axes[r, c].imshow(images[i])\n        axes[r, c].text(0, 0, f\"{float(T[i]):2.2f}\", fontsize=24)\n\nplt.subplots_adjust(wspace=.05, hspace=.05)\nplt.grid(False)\nplt.show()\n\n\n\n\nCool! Looks like we got an image in between A and B that seems to makes sense. Let’s do more steps and see what that looks like…\nThis time, we’ll save images as we go, and then stitch them back together as a gif with ffmpeg.\n\nfrom pathlib import Path\n\noutput_dir = Path('images')\noutput_dir.mkdir(exist_ok=True, parents=True)\n\n# Using same prompt for each image\nprompt = 'blueberry spaghetti'\n\n# Steps to interpolate (i.e. number of images to generate)\nnum_interpolation_steps = 10\nT = torch.linspace(0.0, 1.0, num_interpolation_steps)\n\nimages = []\nfor i, t in enumerate(T):\n    noise_t = slerp(float(t), noise_a, noise_b)\n    im = pipeline(prompt, latents=noise_t, height=height, width=width)['sample'][0]\n    im.save(output_dir / f'frame{i:06d}.png')\n\nUsing ffmpeg, we can bring these together as a clip.\nHere, we’ll lower the frame rate to 5 frames per second, so we should end up with a 2 second clip containing our 10 generating image frames.\n\n! ffmpeg -f image2 -framerate 5 -i images/frame%06d.png -loop 0 output_1.gif\n\nIt should look like this - pretty cool!\n\n\n\noutput_1.gif"
  },
  {
    "objectID": "posts/sd-music-videos/sd_music_videos.html#interpolating-noise-and-text-embedding-vectors",
    "href": "posts/sd-music-videos/sd_music_videos.html#interpolating-noise-and-text-embedding-vectors",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "Interpolating Noise and Text Embedding Vectors",
    "text": "Interpolating Noise and Text Embedding Vectors\nTo ramp it up a notch, lets see if we can interpolate between images with different prompts. To do that, we’ll need to modify the diffusers StableDiffusionPipeline to accept text embeddings, since we’ll want to provide intermediate text embeddings.\nThe main bit we’re adding is this snippet, where we allow for a text_embeddings kwarg that will override the text prompt input.\n    if text_embeddings is None:\n        if isinstance(prompt, str):\n            batch_size = 1\n        elif isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        # get prompt text embeddings\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n\n        if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n            removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.tokenizer.model_max_length :])\n            print(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n            )\n            text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n        text_embeddings = self.text_encoder(text_input_ids.to(self.device))[0]\n    else:\n        batch_size = text_embeddings.shape[0]\nHere’s the full code for our StableDiffusionWalkPipeline:\n\nimport inspect\nfrom typing import Optional, Union, List, Callable\n\nfrom diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n\nclass StableDiffusionWalkPipeline(StableDiffusionPipeline):\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Optional[Union[str, List[str]]] = None,\n        height: int = 512,\n        width: int = 512,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        eta: float = 0.0,\n        generator: Optional[torch.Generator] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n        text_embeddings: Optional[torch.FloatTensor] = None,\n    ):\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n        if text_embeddings is None:\n            if isinstance(prompt, str):\n                batch_size = 1\n            elif isinstance(prompt, list):\n                batch_size = len(prompt)\n            else:\n                raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n            # get prompt text embeddings\n            text_inputs = self.tokenizer(\n                prompt,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                return_tensors=\"pt\",\n            )\n            text_input_ids = text_inputs.input_ids\n\n            if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n                removed_text = self.tokenizer.batch_decode(text_input_ids[:, self.tokenizer.model_max_length :])\n                print(\n                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n                )\n                text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n            text_embeddings = self.text_encoder(text_input_ids.to(self.device))[0]\n        else:\n            batch_size = text_embeddings.shape[0]\n\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance:\n            # HACK - Not setting text_input_ids here when walking, so hard coding to max length of tokenizer\n            # TODO - Determine if this is OK to do\n            # max_length = text_input_ids.shape[-1]\n            max_length = self.tokenizer.model_max_length\n            uncond_input = self.tokenizer(\n                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n            )\n            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n        # get the initial random noise unless the user supplied it\n\n        # Unlike in other pipelines, latents need to be generated in the target device\n        # for 1-to-1 results reproducibility with the CompVis implementation.\n        # However this currently doesn't work in `mps`.\n        latents_device = \"cpu\" if self.device.type == \"mps\" else self.device\n        latents_shape = (batch_size, self.unet.in_channels, height // 8, width // 8)\n        if latents is None:\n            latents = torch.randn(\n                latents_shape,\n                generator=generator,\n                device=latents_device,\n                dtype=text_embeddings.dtype,\n            )\n        else:\n            if latents.shape != latents_shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n            latents = latents.to(latents_device)\n\n        # set timesteps\n        self.scheduler.set_timesteps(num_inference_steps)\n\n        # Some schedulers like PNDM have timesteps as arrays\n        # It's more optimized to move all timesteps to correct device beforehand\n        timesteps_tensor = self.scheduler.timesteps.to(self.device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        for i, t in enumerate(self.progress_bar(timesteps_tensor)):\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n            # predict the noise residual\n            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n            # perform guidance\n            if do_classifier_free_guidance:\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n            # compute the previous noisy sample x_t -> x_t-1\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n            # call the callback, if provided\n            if callback is not None and i % callback_steps == 0:\n                callback(i, t, latents)\n\n        latents = 1 / 0.18215 * latents\n        image = self.vae.decode(latents).sample\n\n        image = (image / 2 + 0.5).clamp(0, 1)\n        image = image.cpu().permute(0, 2, 3, 1).numpy()\n\n        safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n        image, has_nsfw_concept = self.safety_checker(\n            images=image, clip_input=safety_checker_input.pixel_values.to(text_embeddings.dtype)\n        )\n\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n\n        if not return_dict:\n            return (image, has_nsfw_concept)\n\n        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n\nRemove existing pipeline instance before proceeding…\n\ndel pipeline\ntorch.cuda.empty_cache()\n\nThen, initialize the pipeline just as we did before\n\npipeline = StableDiffusionWalkPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\",\n    revision='fp16'\n).to(\"cuda\")\n\nGreat! Now we’ll interpolate between text embeddings. We use torch.lerp instead of slerp, as that’s what we’ve found to be a bit smoother for text.\nWe’ll start by creating two helper functions, embed_text and get_noise so this repetitive code doesn’t muddy up our logic below.\n\ndef embed_text(pipeline, text):\n    \"\"\"takes in text and turns it into text embeddings\"\"\"\n    text_input = pipeline.tokenizer(\n        text,\n        padding=\"max_length\",\n        max_length=pipeline.tokenizer.model_max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    with torch.no_grad():\n        embed = pipeline.text_encoder(text_input.input_ids.to(pipeline.device))[0]\n    return embed\n\ndef get_noise(pipeline, seed, height=512, width=512):\n    \"\"\"Takes in random seed and returns corresponding noise vector\"\"\"\n    return torch.randn(\n        (1, pipeline.unet.in_channels, height // 8, width // 8),\n        generator=torch.Generator(\n            device=pipeline.device\n        ).manual_seed(seed),\n        device=pipeline.device,\n    )\n\n\n# Height and width of image are important for noise vector creation\n# Values should be divisible by 8 if less than 512\n# Values should be divisible by 64 if greater than 512\nheight, width = 512, 512\n\n# Prompts/random seeds for A and B\nprompt_a, prompt_b = 'blueberry spaghetti', 'strawberry spaghetti'\nseed_a, seed_b = 42, 1337\n\n# Noise for A and B\nnoise_a = get_noise(pipeline, seed_a, height=height, width=width)\nnoise_b = get_noise(pipeline, seed_b, height=height, width=width)\n\n# Text embeddings for A and B\nembed_a = embed_text(pipeline, prompt_a)\nembed_b = embed_text(pipeline, prompt_b)\n\n\nfrom pathlib import Path\n\noutput_dir = Path('images_walk_with_text')\noutput_dir.mkdir(exist_ok=True, parents=True)\n\n# Steps to interpolate (i.e. number of images to generate)\nnum_interpolation_steps = 10\nT = torch.linspace(0.0, 1.0, num_interpolation_steps).to(pipeline.device)\n\nimages = []\nfor i, t in enumerate(T):\n    noise_t = slerp(float(t), noise_a, noise_b)\n    embed_t = torch.lerp(embed_a, embed_b, t)\n    im = pipeline(\n        text_embeddings=embed_t,\n        latents=noise_t,\n        height=height,\n        width=width\n    )['sample'][0]\n    im.save(output_dir / f'frame{i:06d}.png')\n\n\n! ffmpeg -f image2 -framerate 5 -i images_walk_with_text/frame%06d.png -loop 0 output_2.gif\n\n\n\n\noutput_2.gif"
  },
  {
    "objectID": "posts/sd-music-videos/sd_music_videos.html#interpolating-to-the-beat-of-a-song",
    "href": "posts/sd-music-videos/sd_music_videos.html#interpolating-to-the-beat-of-a-song",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "Interpolating to the Beat of a Song",
    "text": "Interpolating to the Beat of a Song\nNow we’re talking! But how might we now move this video to the beat of a song?\nAbove, we were moving between images linearly. What we want to do is:\n\nmove more when the energy of a given audio clip is high (it’s loud)\nmove less when the energy is low (it’s quiet).\n\nWe can achieve this by manipulating the weights at certain timesteps that we defined above as T. Instead of using torch.linspace, we’ll try to set these values based on some audio.\nHere we define a helper function to visualize numpy arrays. We’ll use this to help explain what we’re doing.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef plot_array(y):\n    x = np.arange(y.shape[0]) \n    \n    # plotting\n\n    plt.title(\"Line graph\") \n\n    plt.xlabel(\"X axis\") \n\n    plt.ylabel(\"Y axis\") \n\n    plt.plot(x, y, color =\"red\") \n    return plt.show()\n\nNow let’s load in an audio clip. The one we’re using is the choice example clip from librosa.\nIt’s a good one because it has drums and bass in it, so it’s similar to a song you might want to use (but doesn’t involve us using copyrighted music in this notebook 😉).\nWe’ll slice the audio clip so we are only using audio from 0:11-0:14 in the audio.\n\nimport librosa\nfrom IPython.display import Audio\n\nn_mels = 512\nfps = 12\noffset = 11\nduration = 3\n\nwav, sr = librosa.load(librosa.example('choice'), offset=offset, duration=duration)\nAudio(wav, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nLet’s take a look at the plot of the waveform:\n\nplot_array(wav)\n\n\n\n\nAfter much experimentation, I found that extracting the percussive elements from the song and using those for everything moving forward leads to the best results.\nWe’ll do this using the librosa.effects.hpss function.\n\nwav_harmonic, wav_percussive = librosa.effects.hpss(wav, margin=(1.0, 5.0))\nplot_array(wav_percussive)\n\n\n\n\nAs you can see, now the points of percussive impact are more pronounced.\nWhat we’ll do next is:\n\nConvert that audio to spectrogram\nNormalize the spectrogram\nRescale the spectrogram to be (duration * fps) so we have a vector that’s the same length as the amount of frames we wish to generate.\nPlot the resulting array so we can see what it looks like\n\n\n# Number of audio samples per frame\nframe_duration = int(sr / fps)\n\n# Generate Mel Spectrogram\nspec_raw = librosa.feature.melspectrogram(y=wav_percussive, sr=sr, n_mels=n_mels, hop_length=frame_duration)\n\n# Obtain maximum value per time-frame\nspec_max = np.amax(spec_raw, axis=0)\n\n# Normalize all values between 0 and 1\nspec_norm = (spec_max - np.min(spec_max)) / np.ptp(spec_max)\n\n# rescale so its exactly the number of frames we want to generate\n# 3 seconds at 12 fps == 36\namplitude_arr = np.resize(spec_norm, int(duration * fps))\n\nplot_array(amplitude_arr)\n\n\n\n\nFinally, we’ll construct T. We could do this in a variety of ways, but the simplest we found was using np.cumsum to gather a cumulative sum of the “energy” in the audio array.\nHat tip to @teddykoker who helped me figure this out.\n\n# Cumulative sum of audio energy\nT = np.cumsum(amplitude_arr)\n\n# Normalize values of T against last element\nT /= T[-1]\n\n# 0th element not always exactly 0.0. Enforcing that here.\nT[0] = 0.0\n\nplot_array(T)\n\n\n\n\nCompare the above T with our previous definition of T…it’s a lot different!\nWe can see the one above is increasing rapidly at points of high energy, while the one below is simply linear.\n\nplot_array(np.linspace(0.0, 1.0, fps*duration))\n\n\n\n\nLet’s use our newly defined T to generate our music video!\n📝 Note - This cell will take a little while as it has to do the number of steps you see in X-axis above (36 frames)\n\nfrom pathlib import Path\n\noutput_dir = Path('images_walk_with_audio')\noutput_dir.mkdir(exist_ok=True, parents=True)\n\nfor i, t in enumerate(T):\n    noise_t = slerp(float(t), noise_a, noise_b)\n    embed_t = torch.lerp(embed_a, embed_b, t)\n    im = pipeline(\n        text_embeddings=embed_t,\n        latents=noise_t,\n        height=height,\n        width=width\n    )['sample'][0]\n    im.save(output_dir / f'frame{i:06d}.png')\n\nLet’s stitch together the frames we just made as well as the audio clip that goes with it.\nFirst, we’ll write that audio clip to a new file, audio.wav.\n\nimport soundfile as sf\n\nsf.write(output_dir / 'audio.wav', wav, samplerate=sr)\n\nThen, we’ll use some ffmpeg witchcraft to stitch the frames together into an mp4, including the audio clip we just wrote.\n\n! ffmpeg \\\n  -r {fps} \\\n  -i images_walk_with_audio/frame%06d.png \\\n  -i images_walk_with_audio/audio.wav \\\n  -c copy \\\n  -map 0:v:0 \\\n  -map 1:a:0 \\\n  -acodec aac \\\n  -vcodec libx264 \\\n  -pix_fmt yuv420p \\\n  output_walk_with_audio.mp4\n\nLet’s take a look at the video we just made. In Colab, you’ll have to run some code like this. Otherwise, you can just download the file and open it on your computer.\n\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\ndef visualize_video_colab(video_path):\n    mp4 = open(video_path,'rb').read()\n    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n    return HTML(\"\"\"\n    <video width=400 controls>\n        <source src=\"%s\" type=\"video/mp4\">\n    </video>\n    \"\"\" % data_url)\n\nvisualize_video_colab('output_walk_with_audio.mp4')\n\n\n    \n        \n    \n    \n\n\nSuccess!! 🔥"
  },
  {
    "objectID": "posts/sd-music-videos/sd_music_videos.html#parting-tips",
    "href": "posts/sd-music-videos/sd_music_videos.html#parting-tips",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "Parting Tips",
    "text": "Parting Tips\n\nThe quality of the interpolation is better with higher fps. I’ve been using 30 for most of my serious runs, but 60 is probably even better.\nThe workflow I tend to use is:\n\nGenerate images with random seeds, saving them along with the seeds I used.\nPick prompt/seed pairs you like, and use those to generate the videos. That way you know at a high level what the video will look like.\n\nYou can mess with the margin kwarg in librosa.effects.hpss to increase/decrease the intensity of the effect we made here.\nThere are TONs of ways to do what we did here, this is just one way. Feel free to experiment with creating your own T.\nThe NSFW filter can be too liberal at times when generating videos. I tend to remove it when I’m generating videos for myself, but it’s a good idea to keep it in when you’re sharing your videos with others. Please do this responsibly."
  },
  {
    "objectID": "posts/sd-music-videos/sd_music_videos.html#conclusion",
    "href": "posts/sd-music-videos/sd_music_videos.html#conclusion",
    "title": "Creating AI Music Videos with Stable Diffusion",
    "section": "Conclusion",
    "text": "Conclusion\nToday we saw how to create music videos using Stable Diffusion! If you want a nice interface for doing this, you should check out the Stable Diffusion Videos repo, where all of this is wrapped up nicely for you (Check the README for instructions on how to use it for music videos).\nIf you found this notebook helpful:\n\nconsider giving this repo a star ⭐️\nconsider following me on Github @nateraw\n\nThanks for reading! Cheers 🍻"
  }
]